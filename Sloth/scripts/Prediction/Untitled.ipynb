{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgleason/.local/lib/python3.5/site-packages/pyramid/__init__.py:68: UserWarning: \n",
      "    The 'pyramid' package will be migrating to a new namespace beginning in \n",
      "    version 1.0.0: 'pmdarima'. This is due to a package name collision with the\n",
      "    Pyramid web framework. For more information, see Issue #34:\n",
      "    \n",
      "        https://github.com/tgsmith61591/pyramid/issues/34\n",
      "        \n",
      "    The package will subsequently be installable via the name 'pmdarima'; the\n",
      "    only functional change to the user will be the import name. All imports\n",
      "    from 'pyramid' will change to 'pmdarima'.\n",
      "    \n",
      "  \"\"\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from pyramid.arima import auto_arima\n",
    "\n",
    "from Sloth.Sloth import Sloth\n",
    "Sloth = Sloth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'datasets/index.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80a00703de86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/index.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_ind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'datasets/index.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# read and preprocess data\n",
    "data_index = pd.read_csv(\"datasets/index.csv\")\n",
    "data_index['start_ind'][0] = 0\n",
    "data = pd.read_csv(\"datasets/data.csv\")\n",
    "\n",
    "# bin tweets by hour\n",
    "tweet_index = 0\n",
    "s_per_hr = 3600\n",
    "s_per_min = 60\n",
    "s_per_10_min = 360\n",
    "train_splits = [x / 100 for x in range(1,11,1)]\n",
    "retweets = data['relative_time_second'][data_index['start_ind'][tweet_index] + 1:data_index['end_ind'][tweet_index]]\n",
    "hourly = pd.cut(retweets, range(int(retweets.values[0]), int(retweets.values[len(retweets.values) - 1]) + s_per_hr,s_per_hr))\n",
    "bin_hourly = retweets.groupby(hourly).agg('count')\n",
    "#bin_hourly = Sloth.ScaleSeriesMinMax(bin_hourly, -1, 1)[0]\n",
    "bin_hourly = pd.DataFrame(data = bin_hourly.values, index = range(len(bin_hourly)))\n",
    "print(bin_hourly.shape)\n",
    "bin_hourly.columns = ['tweet_count']\n",
    "print(bin_hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that plots time series data\n",
    "# parameters:\n",
    "#     input_data:                input data frame to plot\n",
    "#     title:                     string to represent time series data, to be used as title / ylabel\n",
    "\n",
    "def plot_data(input_data, title):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot(input_data.index, input_data.values, \"k-\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(title)\n",
    "    plt.title(title)\n",
    " \n",
    "# function that plots time series seasonal decomposition\n",
    "# parameters:\n",
    "#     input_data:                input data frame \n",
    "#     frequency:                 frequency, or periodicity, of the time series\n",
    "def plot_seasonal(input_data, *frequency):\n",
    "    if not frequency:\n",
    "        result = Sloth.DecomposeSeriesSeasonal(input_data.index, input_data.values)\n",
    "    else:\n",
    "        result = Sloth.DecomposeSeriesSeasonal(input_data.index, input_data.values, frequency[0])\n",
    "    fig = result.plot()\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.show()\n",
    "\n",
    "# function that makes a future forecast for time series data\n",
    "# parameters:\n",
    "#     train:                     input training data frame\n",
    "#     test:                      input testing data frame\n",
    "#     seasonal:                  boolean; whether data has seasonal component\n",
    "#     seasonal_differencing      period for seasonal differencing\n",
    "\n",
    "def future_forecast(train, test, seasonal, *seasonal_differencing):\n",
    "    \n",
    "    print(\"DEBUG::the size of test is:\")\n",
    "    print(test.shape)\n",
    "    \n",
    "    '''\n",
    "    if not seasonal_differencing:\n",
    "        future_forecast = Sloth.PredictSeriesARIMA(train,test.shape[0],seasonal)\n",
    "    else:\n",
    "        future_forecast = Sloth.PredictSeriesARIMA(train,test.shape[0],seasonal, seasonal_differencing[0])\n",
    "    '''\n",
    "    if not seasonal_differencing:\n",
    "            stepwise_model = auto_arima(train, start_p=1, start_q=1,\n",
    "                            max_p=5, max_q=5, m=1,\n",
    "                            start_P=1, start_Q = 1, seasonal=seasonal,\n",
    "                            d=None, D=0, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=False, \n",
    "                            stepwise=True)\n",
    "        # specified seasonal differencing parameter\n",
    "    else:\n",
    "        stepwise_model = auto_arima(train, start_p=1, start_q=1,\n",
    "                            max_p=5, max_q=5, m=seasonal_differencing[0],\n",
    "                            start_P=1, start_Q = 1, seasonal=seasonal,\n",
    "                            d=None, D=0, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=False, \n",
    "                            stepwise=True)\n",
    "    stepwise_model.fit(train)\n",
    "    future_forecast = stepwise_model.predict(n_periods=test.shape[0])\n",
    "    print(\"DEBUG::Future forecast:\")\n",
    "    print(future_forecast)\n",
    "\n",
    "    future_forecast = pd.DataFrame(future_forecast,index = test.index, columns=[\"Prediction\"])\n",
    "    return future_forecast\n",
    "\n",
    "\n",
    "# function that plots time series data\n",
    "# parameters:\n",
    "#     input_data:                input data frame to plot\n",
    "#     future_forecast:           future forecast for time series data\n",
    "#     title:                     string to represent time series data, to be used as title / ylabel\n",
    "def plot_future_forecast(input_data, test, future_forecast, title):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(pd.concat([test,future_forecast],axis=1).index, pd.concat([test,future_forecast],axis=1).values)\n",
    "    plt.xlabel(\"data point index\")\n",
    "    plt.ylabel(title)\n",
    "    plt.title(title)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(pd.concat([input_data,future_forecast],axis=1).index, pd.concat([input_data,future_forecast],axis=1).values)\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-841f7e5e641d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plots and forecasting for electronic production\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbin_hourly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhourly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#bin_hourly = Sloth.ScaleSeriesMinMax(bin_hourly, -1, 1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbin_hourly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_hourly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_hourly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retweets' is not defined"
     ]
    }
   ],
   "source": [
    "# plots and forecasting for electronic production\n",
    "from scipy import stats\n",
    "bin_hourly = retweets.groupby(hourly).agg('count')\n",
    "#bin_hourly = Sloth.ScaleSeriesMinMax(bin_hourly, -1, 1)[0]\n",
    "bin_hourly = pd.DataFrame(data = bin_hourly.values, index = range(len(bin_hourly)))\n",
    "bin_hourly.columns = ['tweet_count']\n",
    "#bin_hourly['tweet_count'] = [i + 1 for i in bin_hourly['tweet_count']]\n",
    "#bin_hourly_t, lam = stats.boxcox(bin_hourly)\n",
    "#bin_hourly = pd.DataFrame(data = bin_hourly_t, index = range(len(bin_hourly_t)))\n",
    "#bin_hourly['tweet_count'] = [math.log(i) if i > 0 else 0 for i in bin_hourly['tweet_count']]\n",
    "plot_data(bin_hourly, \"Retweet Cascade\")\n",
    "plot_seasonal(bin_hourly)\n",
    "\n",
    "# use 80% of data as training data\n",
    "train_split = int(.5* len(bin_hourly))\n",
    "train = bin_hourly[:train_split]\n",
    "test = bin_hourly[train_split:]\n",
    "future_forecast_tweets = future_forecast(train, test, False, 24)\n",
    "plot_future_forecast(bin_hourly, test, future_forecast_tweets, 'Retweet Cascade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test simple exponential decay example\n",
    "import math\n",
    "\n",
    "X_train = [1000*0.5**i for i in range(10)]\n",
    "X_test = [0 for i in range(10)]\n",
    "data = X_train + X_test + X_test + X_test\n",
    "#data = Sloth.ScaleSeriesMeanVariance(data)[0]\n",
    "#data = [math.log(i) if i > 0 else 0 for i in data]\n",
    "data = [i + 1 for i in data]\n",
    "print(data)\n",
    "data_t, lam = stats.boxcox(data)\n",
    "print(data_t)\n",
    "print(lam)\n",
    "data = pd.DataFrame(data_t)\n",
    "X_train = data[:29]\n",
    "X_test = data[30:]\n",
    "X_test.index = [i + len(X_train) for i in range(len(X_test))]\n",
    "\n",
    "plot_data(data, \"Exponential Decay Example\")\n",
    "plt.show()\n",
    "plot_seasonal(data)\n",
    "\n",
    "future_forecast_exp = future_forecast(X_train, X_test, False)\n",
    "plot_future_forecast(data, X_test, future_forecast_exp, 'Exponential Decay Example')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
